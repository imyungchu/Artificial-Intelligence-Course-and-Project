{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ai_intro_2021_03_01_taxi_problem.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"cells":[{"cell_type":"markdown","metadata":{"id":"lIUGR61n6291"},"source":["Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then run the cells accordingly.\n","\n","Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"]},{"cell_type":"code","metadata":{"id":"77NOrffR6293"},"source":["NAME = \"江詠筑\"\n","COLLABORATORS = \"\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tz446rf06294"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"KRkeSpBGHvj3"},"source":["# Introduce to OpenAI Gym with DuckieNav Environment\n","\n","\n","We will introduce the main API methods in gym:\n","* `reset()`\n","* `step()`\n","* `render()`\n","\n","![hi](https://drive.google.com/uc?id=193KoOaA5YPOha8TnxdHXDXgsciIcSuzV)\n","\n","and the essentials in RL:\n","* Environment\n","* State\n","* Action\n","* Reward\n","* Agent\n","\n","### Environment\n","The example is modifed from the Taxi Problem in \"Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition\" by  Tom Dietterich (2000), Journal of Artificial Intelligence Research.\n","\n","We consider shows a 14 by 9 grid world, except the \"service area.\" The taxi problem is episodic, and in each episode a passenger is located at one of the 4 specially designated locations (R, Y, B, and G). The taxi(agent) starts in a given location and must go to the transported passenger’s location, pick up the passenger, go to the destination location, and put down the passenger. The episode ends when the passenger is deposited at the destination location to one of the 4 locations.\n","\n","```\n","MAP = [\n","    \"+-----------------+\",\n","    \"|O|O| : : : : :G: |\",\n","    \"|O|O| |O| |O| |O| |\",\n","    \"|O| : |O| |O| |O| |\",\n","    \"| : |O|O| : : : : |\",\n","    \"| |O|O|O|O|O| |O| |\",\n","    \"| : :R: : : : |O| |\",\n","    \"| |O|O|O| |O|O|O| |\",\n","    \"| |O| : : |O| : : |\",\n","    \"| |O| |O|O|O|B|O| |\",\n","    \"| : : : : : : |O| |\",\n","    \"| |O| |O| |O| |O| |\",\n","    \"| : : : : : : |O| |\",\n","    \"| |O| |O| |O|O|O| |\",\n","    \"| : : :Y: : : : : |\",\n","    \"+-----------------+\",\n","]\n","```\n","\n","![hi](https://drive.google.com/uc?id=1DRQ5bLGDz_NC-uyuUoPpZRBiCBQgiqsl)\n","<br />\n","\n","\n","\n","\n","Adapted from https://www.oreilly.com/learning/introduction-to-reinforcement-learning-and-openai-gym\n"]},{"cell_type":"markdown","metadata":{"id":"U-dMJmSd5drn"},"source":["## 0. Understand pytest"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PYLGj8Kenic7","executionInfo":{"status":"ok","timestamp":1634533916834,"user_tz":-480,"elapsed":13,"user":{"displayName":"江詠筑","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14201719718375272034"}},"outputId":"b14c8cf2-4f1e-4bfa-ccf5-54d8c616f14c"},"source":["!python --version\n","!pytest --version"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Python 3.7.12\n","This is pytest version 3.6.4, imported from /usr/local/lib/python2.7/dist-packages/pytest.pyc\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jnj1dycC9m6R","executionInfo":{"status":"ok","timestamp":1634533916834,"user_tz":-480,"elapsed":6,"user":{"displayName":"江詠筑","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14201719718375272034"}},"outputId":"dee1b5e3-9b86-4360-9e18-b19908407a15"},"source":["%%file test_math.py\n","\n","import math\n","def test_add():\n","    assert 1+1 == 2\n","\n","def test_mul():\n","    assert 6*7 == 42\n","\n","def test_sin():\n","    assert math.sin(0) == 0"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing test_math.py\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZxdsKosn-LLu","executionInfo":{"status":"ok","timestamp":1634533918340,"user_tz":-480,"elapsed":1088,"user":{"displayName":"江詠筑","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14201719718375272034"}},"outputId":"ff34e648-526f-43a9-aba4-afd48456dffd"},"source":["!python -m pytest test_math.py "],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m============================= test session starts ==============================\u001b[0m\n","platform linux -- Python 3.7.12, pytest-3.6.4, py-1.10.0, pluggy-0.7.1\n","rootdir: /content, inifile:\n","plugins: typeguard-2.7.1\n","\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 3 items                                                             \u001b[0m\u001b[1m\rcollected 3 items                                                              \u001b[0m\n","\n","test_math.py ...\u001b[36m                                                         [100%]\u001b[0m\n","\n","\u001b[32m\u001b[1m=========================== 3 passed in 0.03 seconds ===========================\u001b[0m\n"]}]},{"cell_type":"markdown","metadata":{"id":"5DLVRtaLzIkx"},"source":["## 0. DFS in Froken Lake\n","\n","* S: initial state\n","* F: frozen lake\n","* H: hole\n","* G: the goal\n","* Red square: indicates the current position of the player"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hdwvCv6pzIky","executionInfo":{"status":"ok","timestamp":1634533924199,"user_tz":-480,"elapsed":1776,"user":{"displayName":"江詠筑","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14201719718375272034"}},"outputId":"039e7423-5278-409d-a629-771f79bc0d29"},"source":["import gym # loading the Gym library\n","from gym.envs.toy_text.frozen_lake import generate_random_map\n"," \n","env_fl = gym.make(\"FrozenLake-v0\")\n","env_fl.reset()                    \n","env_fl.render()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n"]}]},{"cell_type":"markdown","metadata":{"id":"6gSJlk5YzIkz"},"source":["### Part A (1 + 1 points)\n","\n","Hint:\n","* You could take a look at https://github.com/openai/gym/blob/master/tests/envs/test_frozenlake_dfs.py\n","* You need to take care of the size of the map in the code."]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"3724ac9d569a77e71370692f39ff6eaf","grade":false,"grade_id":"cell-9961d8a71ced54ab","locked":false,"schema_version":3,"solution":true,"task":false},"id":"hKg6EfnRzIkz"},"source":["def frozenlake_dfs_path_exists(res):\n","    '''\n","    param res: a map (note: you need to take care of size of map in your solution)\n","    out_1: if the dfs path exists\n","    out_2: discovered\n","    '''\n","    frontier, discovered = [], set()\n","    frontier.append((0, 0))\n","    while frontier:\n","      r, c = frontier.pop()\n","      if not (r, c) in discovered:\n","        discovered.add((r, c))\n","        directions = [(1, 0), (0, 1), (-1, 0), (0, -1)]        \n","        for x, y in directions:\n","          r_new = r + x\n","          c_new = c + y\n","          if r_new < 0 or r_new >= len(res) or c_new < 0 or c_new >= len(res[0]) :\n","            continue\n","          if res[r_new][c_new] == \"G\":\n","            return True,discovered\n","          if res[r_new][c_new] not in \"#H\":\n","            frontier.append((r_new, c_new))\n","    return False,discovered\n","\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"1cda199bdf43997071f2b6b98c999eab","grade":true,"grade_id":"cell-ecc89494569a6972","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"id":"BQVMUKwJzIk0"},"source":["map_sizes = [5, 8]\n","for size in map_sizes:\n","    new_frozenlake = generate_random_map(size)\n","    assert len(new_frozenlake) == size\n","    assert len(new_frozenlake[0]) == size\n","    out_1, out_2 = frozenlake_dfs_path_exists(new_frozenlake)\n","    assert out_1 == True"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"d5f139107129b1db137626f7723cbcaf","grade":true,"grade_id":"cell-f02c3efea748afed","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"id":"mzzt-yydzIk0"},"source":["MAPS = {\n","    \"4x4\": [\n","        \"SFFF\", \n","        \"FHFH\", \n","        \"FFFH\", \n","        \"HFFG\"],\n","}\n","out_1, out_2 = frozenlake_dfs_path_exists(MAPS[\"4x4\"])\n","assert out_2 == {(0, 1), (1, 2), (2, 1), (0, 0), (3, 1), (0, 3), (2, 0), (0, 2), (2, 2), (1, 0), (3, 2)}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iTn16GI7Hvj4"},"source":["## 1. Initialize DuckieNav-v2\n","\n","### Installation\n","You can obtain and install this customized gym environment (https://github.com/ARG-NCTU/gym-duckienav.git): \n"]},{"cell_type":"code","metadata":{"id":"KH4xVvH-H3MY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634533981387,"user_tz":-480,"elapsed":6378,"user":{"displayName":"江詠筑","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14201719718375272034"}},"outputId":"0842a465-b74d-4b48-f9f0-e25661957160"},"source":["!git clone https://github.com/ARG-NCTU/gym-duckienav.git\n","%cd gym-duckienav\n","!pip install -e ."],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'gym-duckienav'...\n","remote: Enumerating objects: 104, done.\u001b[K\n","remote: Counting objects: 100% (13/13), done.\u001b[K\n","remote: Compressing objects: 100% (13/13), done.\u001b[K\n","remote: Total 104 (delta 4), reused 3 (delta 0), pack-reused 91\u001b[K\n","Receiving objects: 100% (104/104), 825.81 KiB | 30.58 MiB/s, done.\n","Resolving deltas: 100% (35/35), done.\n","/content/gym-duckienav\n","Obtaining file:///content/gym-duckienav\n","Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (from gym-duckienav==0.0.1) (0.17.3)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym->gym-duckienav==0.0.1) (1.4.1)\n","Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym->gym-duckienav==0.0.1) (1.3.0)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym->gym-duckienav==0.0.1) (1.5.0)\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym->gym-duckienav==0.0.1) (1.19.5)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->gym-duckienav==0.0.1) (0.16.0)\n","Installing collected packages: gym-duckienav\n","  Running setup.py develop for gym-duckienav\n","Successfully installed gym-duckienav-0.0.1\n"]}]},{"cell_type":"code","metadata":{"id":"5TpZj55mHvj5"},"source":["import gym\n","import gym_duckienav\n","import gym.spaces\n","import numpy as np\n","\n","env = gym.make(\"DuckieNav-v2\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8Z9do7BOHvj8"},"source":["### How many 'states' in observation_space: \n","There are 2520 states from: 14 (rows) x 9 (columns) x 5 (passenger locations: R, Y, B, G, or on taxi) x 4 (destinations: R, Y, B, or G)"]},{"cell_type":"code","metadata":{"id":"Km1Mh6FEHvj8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634534478881,"user_tz":-480,"elapsed":311,"user":{"displayName":"江詠筑","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14201719718375272034"}},"outputId":"8fd0bfc1-f6ed-4108-b20e-d4b5d3af8eed"},"source":["env.observation_space.n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2520"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"IgQsJ1AEHvj_"},"source":["### action_space: \n","There are 6 possible actions in Taxi-v2 environment\n","* down (0), up (1), right (2), left (3), pick-up (4), and drop-off (5)"]},{"cell_type":"code","metadata":{"id":"X9b6dfXgHvkA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634534481762,"user_tz":-480,"elapsed":327,"user":{"displayName":"江詠筑","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14201719718375272034"}},"outputId":"bb48f6c9-2556-4cbe-f9e6-dc7ae5d1ee7b"},"source":["env.action_space.n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["6"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"88C3R2sWHvkC"},"source":["## 2. States\n","\n","Resets the state of the environment and returns an initial observation (state).\n","\n","The current state is from :\n","* current taxi row position\n","* current taxi colum position\n","* passenger location (Blue or in taxi) from 0: R, 1: G, 2: Y, 3: B; 4: in taxi.\n","* destination location (Magenta) from 0: R, 1: G, 2: Y, 3: B"]},{"cell_type":"code","metadata":{"id":"wlg8_59eHvkC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634538301637,"user_tz":-480,"elapsed":331,"user":{"displayName":"江詠筑","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14201719718375272034"}},"outputId":"855eec66-0cf3-41fa-ec49-977dc41b720e"},"source":["env.reset()\n","\n","print(\"Current state: \", str(env.s))\n","for p in env.decode(env.s): print(p)\n","env.render()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Current state:  161\n","0\n","8\n","0\n","1\n","+-----------------+\n","|O|O| : : : : :\u001b[35mG\u001b[0m:\u001b[43m \u001b[0m|\n","|O|O| |O| |O| |O| |\n","|O| : |O| |O| |O| |\n","| : |O|O| : : : : |\n","| |O|O|O|O|O| |O| |\n","| : :\u001b[34;1mR\u001b[0m: : : : |O| |\n","| |O|O|O| |O|O|O| |\n","| |O| : : |O| : : |\n","| |O| |O|O|O|B|O| |\n","| : : : : : : |O| |\n","| |O| |O| |O| |O| |\n","| : : : : : : |O| |\n","| |O| |O| |O|O|O| |\n","| : : :Y: : : : : |\n","+-----------------+\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"A-ohwkY3HvkF"},"source":["Repeat previous cell for a few times.\n","\n","In taxi problem, the colors mean:\n","* blue: passenger's current position\n","* magenta: destination\n","* yellow: empty taxi\n","* green: full taxi"]},{"cell_type":"markdown","metadata":{"id":"ouXLc7MFHvkF"},"source":["## 3. Actions\n","\n","Remember that the taxi agent can perform the following actions:\n","* 0: \"South\", \n","* 1: \"North\", \n","* 2: \"East\", \n","* 3: \"West\", \n","* 4: \"Pickup\", \n","* 5: \"Dropoff\"\n","\n","Let's set the state to 124.\n","Let the taxi agent perform some actions.  "]},{"cell_type":"code","metadata":{"id":"Brc6CTo2HvkG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634538311118,"user_tz":-480,"elapsed":334,"user":{"displayName":"江詠筑","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14201719718375272034"}},"outputId":"24f006ae-07c0-4dd6-e915-4eea86f4e17d"},"source":["ACTIONS = [\"South\", \"North\", \"East\", \"West\", \"Pickup\", \"Dropoff\"]\n","env.s = 124\n","env.render()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----------------+\n","|O|O| : : : :\u001b[43m \u001b[0m:\u001b[34;1mG\u001b[0m: |\n","|O|O| |O| |O| |O| |\n","|O| : |O| |O| |O| |\n","| : |O|O| : : : : |\n","| |O|O|O|O|O| |O| |\n","| : :\u001b[35mR\u001b[0m: : : : |O| |\n","| |O|O|O| |O|O|O| |\n","| |O| : : |O| : : |\n","| |O| |O|O|O|B|O| |\n","| : : : : : : |O| |\n","| |O| |O| |O| |O| |\n","| : : : : : : |O| |\n","| |O| |O| |O|O|O| |\n","| : : :Y: : : : : |\n","+-----------------+\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"2rjDHroJHvkI"},"source":["### `step()`\n","\n","Run one timestep of the environment's dynamics. \n","It returns a tuple (observation, reward, done, info)\n","* observation (object): agent's observation of the current environment\n","* reward (float) : amount of reward returned after previous action\n","* done (boolean): whether the episode has ended, in which case further step() calls will return undefined results\n","* info (dict): contains auxiliary diagnostic information (helpful for debugging, and sometimes learning)\n","\n","Essentially the empty taxi is supposed to: \n","* move toward the blue letter, \n","* pickup the passenger (now the taxi is green), \n","* drive to the magenta letter, and \n","* drop the passenger (the taxi is yellow again).\n","\n","It is obvious that we should start with moving \"East\" env.step(2). Index 2 is for moving \"East\"\n","We will do the followings:\n","* Perform \"Pickup\" step(4) (although the passenger is not here)\n","* Perform \"East\" step(2)\n","* Perform \"Pickup\" step(4)\n","* Perform \"West\" step(3)\n","* Perform \"South\" step(0) for 5 times\n","* Perfomr \"Dropoff\" (5)\n","* Perform \"West\" step(3) for 4 times\n","* Perfomr \"Dropoff\" (5)"]},{"cell_type":"code","metadata":{"id":"u4OMQ4rzHvkI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634538316566,"user_tz":-480,"elapsed":325,"user":{"displayName":"江詠筑","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14201719718375272034"}},"outputId":"261b7626-f292-42e8-f078-871ba41c25e2"},"source":["state, reward, done, info = env.step(4)\n","env.render()\n","print(\"reward: \", str(reward))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----------------+\n","|O|O| : : : :\u001b[43m \u001b[0m:\u001b[34;1mG\u001b[0m: |\n","|O|O| |O| |O| |O| |\n","|O| : |O| |O| |O| |\n","| : |O|O| : : : : |\n","| |O|O|O|O|O| |O| |\n","| : :\u001b[35mR\u001b[0m: : : : |O| |\n","| |O|O|O| |O|O|O| |\n","| |O| : : |O| : : |\n","| |O| |O|O|O|B|O| |\n","| : : : : : : |O| |\n","| |O| |O| |O| |O| |\n","| : : : : : : |O| |\n","| |O| |O| |O|O|O| |\n","| : : :Y: : : : : |\n","+-----------------+\n","  (Pickup)\n","reward:  -10\n"]}]},{"cell_type":"code","metadata":{"id":"TYtmsNzpHvkL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634538321667,"user_tz":-480,"elapsed":5,"user":{"displayName":"江詠筑","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14201719718375272034"}},"outputId":"d70a47f5-9e8e-4e82-d4e7-5fe6d40e2377"},"source":["state, reward, done, info = env.step(2)\n","env.render()\n","print(\"reward: \", str(reward))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----------------+\n","|O|O| : : : : :\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m: |\n","|O|O| |O| |O| |O| |\n","|O| : |O| |O| |O| |\n","| : |O|O| : : : : |\n","| |O|O|O|O|O| |O| |\n","| : :\u001b[35mR\u001b[0m: : : : |O| |\n","| |O|O|O| |O|O|O| |\n","| |O| : : |O| : : |\n","| |O| |O|O|O|B|O| |\n","| : : : : : : |O| |\n","| |O| |O| |O| |O| |\n","| : : : : : : |O| |\n","| |O| |O| |O|O|O| |\n","| : : :Y: : : : : |\n","+-----------------+\n","  (East)\n","reward:  -1\n"]}]},{"cell_type":"code","metadata":{"id":"8xGl-lliHvkN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634538325744,"user_tz":-480,"elapsed":319,"user":{"displayName":"江詠筑","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14201719718375272034"}},"outputId":"90449f24-a7ed-475c-871c-53007d4647ec"},"source":["state, reward, done, info = env.step(4)\n","env.render()\n","print(\"reward: \", str(reward))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----------------+\n","|O|O| : : : : :\u001b[42mG\u001b[0m: |\n","|O|O| |O| |O| |O| |\n","|O| : |O| |O| |O| |\n","| : |O|O| : : : : |\n","| |O|O|O|O|O| |O| |\n","| : :\u001b[35mR\u001b[0m: : : : |O| |\n","| |O|O|O| |O|O|O| |\n","| |O| : : |O| : : |\n","| |O| |O|O|O|B|O| |\n","| : : : : : : |O| |\n","| |O| |O| |O| |O| |\n","| : : : : : : |O| |\n","| |O| |O| |O|O|O| |\n","| : : :Y: : : : : |\n","+-----------------+\n","  (Pickup)\n","reward:  -1\n"]}]},{"cell_type":"code","metadata":{"id":"Vz03Qt_8HvkP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634538331279,"user_tz":-480,"elapsed":304,"user":{"displayName":"江詠筑","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14201719718375272034"}},"outputId":"a1ab1b15-dd2a-4547-df95-6e232b9d2fd8"},"source":["state, reward, done, info = env.step(3)\n","env.render()\n","print(\"reward: \", str(reward))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----------------+\n","|O|O| : : : :\u001b[42m_\u001b[0m:G: |\n","|O|O| |O| |O| |O| |\n","|O| : |O| |O| |O| |\n","| : |O|O| : : : : |\n","| |O|O|O|O|O| |O| |\n","| : :\u001b[35mR\u001b[0m: : : : |O| |\n","| |O|O|O| |O|O|O| |\n","| |O| : : |O| : : |\n","| |O| |O|O|O|B|O| |\n","| : : : : : : |O| |\n","| |O| |O| |O| |O| |\n","| : : : : : : |O| |\n","| |O| |O| |O|O|O| |\n","| : : :Y: : : : : |\n","+-----------------+\n","  (West)\n","reward:  -1\n"]}]},{"cell_type":"code","metadata":{"id":"AqpxPit6HvkT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634538336465,"user_tz":-480,"elapsed":312,"user":{"displayName":"江詠筑","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14201719718375272034"}},"outputId":"16990bc8-14d1-4b26-d6a4-908249a8306f"},"source":["for i in range(0, 5):\n","    env.step(0)\n","env.render()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----------------+\n","|O|O| : : : : :G: |\n","|O|O| |O| |O| |O| |\n","|O| : |O| |O| |O| |\n","| : |O|O| : : : : |\n","| |O|O|O|O|O| |O| |\n","| : :\u001b[35mR\u001b[0m: : : :\u001b[42m_\u001b[0m|O| |\n","| |O|O|O| |O|O|O| |\n","| |O| : : |O| : : |\n","| |O| |O|O|O|B|O| |\n","| : : : : : : |O| |\n","| |O| |O| |O| |O| |\n","| : : : : : : |O| |\n","| |O| |O| |O|O|O| |\n","| : : :Y: : : : : |\n","+-----------------+\n","  (South)\n"]}]},{"cell_type":"code","metadata":{"id":"-Iml7WnSHvkV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634538340178,"user_tz":-480,"elapsed":313,"user":{"displayName":"江詠筑","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14201719718375272034"}},"outputId":"7fbd6e59-f309-4dfc-d644-fe1fcfc8c4ea"},"source":["state, reward, done, info = env.step(5)\n","env.render()\n","print(\"reward: \", str(reward))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----------------+\n","|O|O| : : : : :G: |\n","|O|O| |O| |O| |O| |\n","|O| : |O| |O| |O| |\n","| : |O|O| : : : : |\n","| |O|O|O|O|O| |O| |\n","| : :\u001b[35mR\u001b[0m: : : :\u001b[42m_\u001b[0m|O| |\n","| |O|O|O| |O|O|O| |\n","| |O| : : |O| : : |\n","| |O| |O|O|O|B|O| |\n","| : : : : : : |O| |\n","| |O| |O| |O| |O| |\n","| : : : : : : |O| |\n","| |O| |O| |O|O|O| |\n","| : : :Y: : : : : |\n","+-----------------+\n","  (Dropoff)\n","reward:  -10\n"]}]},{"cell_type":"code","metadata":{"id":"wVJEKsSNHvkX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634538375099,"user_tz":-480,"elapsed":316,"user":{"displayName":"江詠筑","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14201719718375272034"}},"outputId":"f944f856-5479-495a-c04e-f75bb98b9a47"},"source":["for i in range(0, 4):\n","    env.step(3)\n","env.render()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----------------+\n","|O|O| : : : : :G: |\n","|O|O| |O| |O| |O| |\n","|O| : |O| |O| |O| |\n","| : |O|O| : : : : |\n","| |O|O|O|O|O| |O| |\n","| : :\u001b[35m\u001b[42mR\u001b[0m\u001b[0m: : : : |O| |\n","| |O|O|O| |O|O|O| |\n","| |O| : : |O| : : |\n","| |O| |O|O|O|B|O| |\n","| : : : : : : |O| |\n","| |O| |O| |O| |O| |\n","| : : : : : : |O| |\n","| |O| |O| |O|O|O| |\n","| : : :Y: : : : : |\n","+-----------------+\n","  (West)\n"]}]},{"cell_type":"code","metadata":{"id":"MO0Lna3IHvka","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634538378140,"user_tz":-480,"elapsed":309,"user":{"displayName":"江詠筑","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14201719718375272034"}},"outputId":"999db44c-4c41-40dc-bca5-2a0451c472f1"},"source":["state, reward, done, info = env.step(5)\n","env.render()\n","print(\"reward: \", str(reward))\n","print(\"done: \", str(done))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----------------+\n","|O|O| : : : : :G: |\n","|O|O| |O| |O| |O| |\n","|O| : |O| |O| |O| |\n","| : |O|O| : : : : |\n","| |O|O|O|O|O| |O| |\n","| : :\u001b[35m\u001b[42mR\u001b[0m\u001b[0m: : : : |O| |\n","| |O|O|O| |O|O|O| |\n","| |O| : : |O| : : |\n","| |O| |O|O|O|B|O| |\n","| : : : : : : |O| |\n","| |O| |O| |O| |O| |\n","| : : : : : : |O| |\n","| |O| |O| |O|O|O| |\n","| : : :Y: : : : : |\n","+-----------------+\n","  (Dropoff)\n","reward:  20\n","done:  True\n"]}]},{"cell_type":"markdown","metadata":{"id":"nfZStqfUHvkc"},"source":["### Rewards\n","\n","You have probably figured out the rewards:\n","* Perform any movements: -1\n","* Pick up or drop off at the wrong position: -10\n","* Drop off the passenger at the right position: 20 "]},{"cell_type":"markdown","metadata":{"id":"d63sNdFzHvkc"},"source":["## 4. Random Agent: \n","\n","We will use the funciton env.action_space.sample(); you could run the following cell for a few times"]},{"cell_type":"code","metadata":{"id":"bsFLsYY9Hvkd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634538381548,"user_tz":-480,"elapsed":433,"user":{"displayName":"江詠筑","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14201719718375272034"}},"outputId":"68ecdc21-72bb-4a61-9413-d93880d30c4e"},"source":["print(env.action_space.sample())"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2\n"]}]},{"cell_type":"markdown","metadata":{"id":"Hymp7jErHvkf"},"source":["### How good does behaving completely random do?"]},{"cell_type":"code","metadata":{"id":"3hHlDYUJHvkf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634538383394,"user_tz":-480,"elapsed":304,"user":{"displayName":"江詠筑","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14201719718375272034"}},"outputId":"c803e90b-1fb7-4cbc-c9f2-986ae091fe91"},"source":["state = env.reset()\n","\n","counter = 0\n","g = 0\n","reward = None\n","while reward != 20:\n","    state, reward, done, info = env.step(env.action_space.sample())\n","    counter += 1\n","    g += reward\n","print(\"Solved in {} Steps with a total reward of {}\".format(counter,g))\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Solved in 703 Steps with a total reward of -18103\n"]}]},{"cell_type":"markdown","metadata":{"id":"u1gWBuUyHvkh"},"source":["You may luck out and solve the environment fairly quickly, but on average, a completely random policy will solve this environment in about ???? steps"]},{"cell_type":"markdown","metadata":{"id":"U3z06QSlHvkh"},"source":["## 5. Agent with Basic Reinforcement Learning: Q-Learning\n","\n","In order to maximize our reward, we will have to have the algorithm remember its actions and their associated rewards. Here, the algorithm’s memory is going to be a Q action value table.\n","\n","To manage this Q table, we will use a NumPy array. The size of this table will be the number of states (2520) by the number of possible actions (6)."]},{"cell_type":"code","metadata":{"id":"E0aRwMppHvki","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634538388984,"user_tz":-480,"elapsed":1537,"user":{"displayName":"江詠筑","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14201719718375272034"}},"outputId":"08fcaa36-9eae-49f8-c0c8-bbe4d5f16477"},"source":["n_states = env.observation_space.n\n","n_actions = env.action_space.n\n","Q = np.zeros([n_states, n_actions])\n","ACTIONS = [\"South\", \"North\", \"East\", \"West\", \"Pickup\", \"Dropoff\"]\n","\n","episodes = 1\n","G = 0\n","counter = 0\n","alpha = 0.618\n","\n","for episode in range(1,episodes+1):\n","    done = False\n","    G, reward = 0,0\n","\n","    state = env.reset()\n","    env.render()\n","    print(\"Step: {}, Action: {}, Reward: {}, Q[{}] \\t{}\".format(\"   \", \"   \", \"   \", \"   \", ACTIONS))\n","\n","    firstState = state\n","    print(\"Initial State = {}\".format(state))\n","    while reward != 20:\n","        action = np.argmax(Q[state])  #1\n","        state2, reward, done, info = env.step(action) #2\n","\n","        if counter < 20:\n","            print(\"Before Step: {}, Action: {}, Reward: {}, Q[{}] \\t{}\".format(counter, ACTIONS[action], reward, state, Q[state]))\n","            print(\"Future Step: {}, Action: {}, Reward: {}, Q[{}] \\t{}\".format(counter, ACTIONS[action], reward, state2, Q[state2]))\n","\n","        Q[state,action] += alpha * (reward + np.max(Q[state2]) - Q[state,action]) #3\n","        \n","        if counter < 20:\n","            print(\"After  Step: {}, Action: {}, Reward: {}, Q[{}] \\t{}\".format(counter, ACTIONS[action], reward, state, Q[state]))\n","            print(\"--------\")\n","\n","        counter += 1\n","        G += reward\n","        state = state2\n","        \n","finalState = state\n","print(\"Final State = {}\".format(finalState))\n","print(\"Solved in {} Steps with a total reward of {}\".format(counter, G))\n","env.render()\n","\n","print(ACTIONS)\n","print(Q[finalState])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----------------+\n","|O|O| : : :\u001b[43m \u001b[0m: :\u001b[35mG\u001b[0m: |\n","|O|O| |O| |O| |O| |\n","|O| : |O| |O| |O| |\n","| : |O|O| : : : : |\n","| |O|O|O|O|O| |O| |\n","| : :R: : : : |O| |\n","| |O|O|O| |O|O|O| |\n","| |O| : : |O| : : |\n","| |O| |O|O|O|B|O| |\n","| : : : : : : |O| |\n","| |O| |O| |O| |O| |\n","| : : : : : : |O| |\n","| |O| |O| |O|O|O| |\n","| : : :\u001b[34;1mY\u001b[0m: : : : : |\n","+-----------------+\n","\n","Step:    , Action:    , Reward:    , Q[   ] \t['South', 'North', 'East', 'West', 'Pickup', 'Dropoff']\n","Initial State = 109\n","Before Step: 0, Action: South, Reward: -50, Q[109] \t[0. 0. 0. 0. 0. 0.]\n","Future Step: 0, Action: South, Reward: -50, Q[289] \t[0. 0. 0. 0. 0. 0.]\n","After  Step: 0, Action: South, Reward: -50, Q[109] \t[-30.9   0.    0.    0.    0.    0. ]\n","--------\n","Before Step: 1, Action: South, Reward: -50, Q[289] \t[0. 0. 0. 0. 0. 0.]\n","Future Step: 1, Action: South, Reward: -50, Q[469] \t[0. 0. 0. 0. 0. 0.]\n","After  Step: 1, Action: South, Reward: -50, Q[289] \t[-30.9   0.    0.    0.    0.    0. ]\n","--------\n","Before Step: 2, Action: South, Reward: -1, Q[469] \t[0. 0. 0. 0. 0. 0.]\n","Future Step: 2, Action: South, Reward: -1, Q[649] \t[0. 0. 0. 0. 0. 0.]\n","After  Step: 2, Action: South, Reward: -1, Q[469] \t[-0.618  0.     0.     0.     0.     0.   ]\n","--------\n","Before Step: 3, Action: South, Reward: -50, Q[649] \t[0. 0. 0. 0. 0. 0.]\n","Future Step: 3, Action: South, Reward: -50, Q[829] \t[0. 0. 0. 0. 0. 0.]\n","After  Step: 3, Action: South, Reward: -50, Q[649] \t[-30.9   0.    0.    0.    0.    0. ]\n","--------\n","Before Step: 4, Action: South, Reward: -1, Q[829] \t[0. 0. 0. 0. 0. 0.]\n","Future Step: 4, Action: South, Reward: -1, Q[1009] \t[0. 0. 0. 0. 0. 0.]\n","After  Step: 4, Action: South, Reward: -1, Q[829] \t[-0.618  0.     0.     0.     0.     0.   ]\n","--------\n","Before Step: 5, Action: South, Reward: -50, Q[1009] \t[0. 0. 0. 0. 0. 0.]\n","Future Step: 5, Action: South, Reward: -50, Q[1189] \t[0. 0. 0. 0. 0. 0.]\n","After  Step: 5, Action: South, Reward: -50, Q[1009] \t[-30.9   0.    0.    0.    0.    0. ]\n","--------\n","Before Step: 6, Action: South, Reward: -50, Q[1189] \t[0. 0. 0. 0. 0. 0.]\n","Future Step: 6, Action: South, Reward: -50, Q[1369] \t[0. 0. 0. 0. 0. 0.]\n","After  Step: 6, Action: South, Reward: -50, Q[1189] \t[-30.9   0.    0.    0.    0.    0. ]\n","--------\n","Before Step: 7, Action: South, Reward: -50, Q[1369] \t[0. 0. 0. 0. 0. 0.]\n","Future Step: 7, Action: South, Reward: -50, Q[1549] \t[0. 0. 0. 0. 0. 0.]\n","After  Step: 7, Action: South, Reward: -50, Q[1369] \t[-30.9   0.    0.    0.    0.    0. ]\n","--------\n","Before Step: 8, Action: South, Reward: -1, Q[1549] \t[0. 0. 0. 0. 0. 0.]\n","Future Step: 8, Action: South, Reward: -1, Q[1729] \t[0. 0. 0. 0. 0. 0.]\n","After  Step: 8, Action: South, Reward: -1, Q[1549] \t[-0.618  0.     0.     0.     0.     0.   ]\n","--------\n","Before Step: 9, Action: South, Reward: -50, Q[1729] \t[0. 0. 0. 0. 0. 0.]\n","Future Step: 9, Action: South, Reward: -50, Q[1909] \t[0. 0. 0. 0. 0. 0.]\n","After  Step: 9, Action: South, Reward: -50, Q[1729] \t[-30.9   0.    0.    0.    0.    0. ]\n","--------\n","Before Step: 10, Action: South, Reward: -1, Q[1909] \t[0. 0. 0. 0. 0. 0.]\n","Future Step: 10, Action: South, Reward: -1, Q[2089] \t[0. 0. 0. 0. 0. 0.]\n","After  Step: 10, Action: South, Reward: -1, Q[1909] \t[-0.618  0.     0.     0.     0.     0.   ]\n","--------\n","Before Step: 11, Action: South, Reward: -50, Q[2089] \t[0. 0. 0. 0. 0. 0.]\n","Future Step: 11, Action: South, Reward: -50, Q[2269] \t[0. 0. 0. 0. 0. 0.]\n","After  Step: 11, Action: South, Reward: -50, Q[2089] \t[-30.9   0.    0.    0.    0.    0. ]\n","--------\n","Before Step: 12, Action: South, Reward: -50, Q[2269] \t[0. 0. 0. 0. 0. 0.]\n","Future Step: 12, Action: South, Reward: -50, Q[2449] \t[0. 0. 0. 0. 0. 0.]\n","After  Step: 12, Action: South, Reward: -50, Q[2269] \t[-30.9   0.    0.    0.    0.    0. ]\n","--------\n","Before Step: 13, Action: South, Reward: -50, Q[2449] \t[0. 0. 0. 0. 0. 0.]\n","Future Step: 13, Action: South, Reward: -50, Q[2449] \t[0. 0. 0. 0. 0. 0.]\n","After  Step: 13, Action: South, Reward: -50, Q[2449] \t[-30.9   0.    0.    0.    0.    0. ]\n","--------\n","Before Step: 14, Action: North, Reward: -50, Q[2449] \t[-30.9   0.    0.    0.    0.    0. ]\n","Future Step: 14, Action: North, Reward: -50, Q[2269] \t[-30.9   0.    0.    0.    0.    0. ]\n","After  Step: 14, Action: North, Reward: -50, Q[2449] \t[-30.9 -30.9   0.    0.    0.    0. ]\n","--------\n","Before Step: 15, Action: North, Reward: -1, Q[2269] \t[-30.9   0.    0.    0.    0.    0. ]\n","Future Step: 15, Action: North, Reward: -1, Q[2089] \t[-30.9   0.    0.    0.    0.    0. ]\n","After  Step: 15, Action: North, Reward: -1, Q[2269] \t[-30.9    -0.618   0.      0.      0.      0.   ]\n","--------\n","Before Step: 16, Action: North, Reward: -50, Q[2089] \t[-30.9   0.    0.    0.    0.    0. ]\n","Future Step: 16, Action: North, Reward: -50, Q[1909] \t[-0.618  0.     0.     0.     0.     0.   ]\n","After  Step: 16, Action: North, Reward: -50, Q[2089] \t[-30.9 -30.9   0.    0.    0.    0. ]\n","--------\n","Before Step: 17, Action: North, Reward: -1, Q[1909] \t[-0.618  0.     0.     0.     0.     0.   ]\n","Future Step: 17, Action: North, Reward: -1, Q[1729] \t[-30.9   0.    0.    0.    0.    0. ]\n","After  Step: 17, Action: North, Reward: -1, Q[1909] \t[-0.618 -0.618  0.     0.     0.     0.   ]\n","--------\n","Before Step: 18, Action: North, Reward: -50, Q[1729] \t[-30.9   0.    0.    0.    0.    0. ]\n","Future Step: 18, Action: North, Reward: -50, Q[1549] \t[-0.618  0.     0.     0.     0.     0.   ]\n","After  Step: 18, Action: North, Reward: -50, Q[1729] \t[-30.9 -30.9   0.    0.    0.    0. ]\n","--------\n","Before Step: 19, Action: North, Reward: -50, Q[1549] \t[-0.618  0.     0.     0.     0.     0.   ]\n","Future Step: 19, Action: North, Reward: -50, Q[1369] \t[-30.9   0.    0.    0.    0.    0. ]\n","After  Step: 19, Action: North, Reward: -50, Q[1549] \t[ -0.618 -30.9     0.      0.      0.      0.   ]\n","--------\n","Final State = 157\n","Solved in 32081 Steps with a total reward of -140286\n","+-----------------+\n","|O|O| : : : : :\u001b[35m\u001b[42mG\u001b[0m\u001b[0m: |\n","|O|O| |O| |O| |O| |\n","|O| : |O| |O| |O| |\n","| : |O|O| : : : : |\n","| |O|O|O|O|O| |O| |\n","| : :R: : : : |O| |\n","| |O|O|O| |O|O|O| |\n","| |O| : : |O| : : |\n","| |O| |O|O|O|B|O| |\n","| : : : : : : |O| |\n","| |O| |O| |O| |O| |\n","| : : : : : : |O| |\n","| |O| |O| |O|O|O| |\n","| : : :Y: : : : : |\n","+-----------------+\n","  (Dropoff)\n","['South', 'North', 'East', 'West', 'Pickup', 'Dropoff']\n","[-30.9   -30.9   -30.9    -0.618  -6.18   12.36 ]\n"]}]},{"cell_type":"markdown","metadata":{"id":"3z4zHwr9Hvkk"},"source":["First (#1): The agent starts by choosing an action with the highest Q value for the current state using argmax. Argmax will return the index/action with the highest value for that state. Initially, our Q table will be all zeros. But, after every step, the Q values for state-action pairs will be updated.\n","\n","Second (#2): The agent then takes action and we store the future state as state2 (S t+1). This will allow the agent to compare the previous state to the new state.\n","\n","Third (#3): We update the state-action pair (St , At) for Q using the reward, and the max Q value for state2 (S t+1). This update is done using the action value formula (based upon the Bellman equation) and allows state-action pairs to be updated in a recursive fashion (based on future values). See the following Figure for the value iteration update.\n","\n","![hi](https://drive.google.com/uc?id=1QqKZbyJagZ6laGb06dDFrj55zdjUj107)\n","<br />"]},{"cell_type":"code","metadata":{"id":"f20T_lSs07xC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634538447615,"user_tz":-480,"elapsed":304,"user":{"displayName":"江詠筑","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14201719718375272034"}},"outputId":"b6085d6e-df2e-4450-a4d3-b2ce60ed876b"},"source":["print(str(np.count_nonzero(Q)), \" / \", str(n_states * n_actions))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2814  /  15120\n"]}]},{"cell_type":"markdown","metadata":{"id":"7HGO6aJEHvkk"},"source":["### Let's run over multiple episodes so that we can converge on an optimal policy"]},{"cell_type":"code","metadata":{"id":"xKycyzrcHvkk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634538471353,"user_tz":-480,"elapsed":13056,"user":{"displayName":"江詠筑","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14201719718375272034"}},"outputId":"db4ccc87-85d3-4ff8-fe44-8a0a793e6734"},"source":["episodes = 10000\n","rewardTracker = []\n","\n","G = 0\n","alpha = 0.618\n","\n","for episode in range(1,episodes+1):\n","    done = False\n","    G, reward = 0,0\n","\n","    state = env.reset()\n","\n","    while done != True:\n","        action = np.argmax(Q[state]) \n","        state2, reward, done, info = env.step(action) \n","        Q[state,action] += alpha * ((reward + (np.max(Q[state2]))  - Q[state,action]))\n","        G += reward\n","        state = state2\n","    \n","    if episode % 100 == 0:\n","        print('Episode {} Total Reward: {}'.format(episode,G))\n","    \n","print(str(np.count_nonzero(Q)), \" / \", str(n_states * n_actions))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Episode 100 Total Reward: -31\n","Episode 200 Total Reward: -2113\n","Episode 300 Total Reward: -109\n","Episode 400 Total Reward: -43\n","Episode 500 Total Reward: -135\n","Episode 600 Total Reward: -62\n","Episode 700 Total Reward: -293\n","Episode 800 Total Reward: -46\n","Episode 900 Total Reward: -10\n","Episode 1000 Total Reward: -47\n","Episode 1100 Total Reward: 5\n","Episode 1200 Total Reward: -45\n","Episode 1300 Total Reward: -67\n","Episode 1400 Total Reward: -60\n","Episode 1500 Total Reward: -49\n","Episode 1600 Total Reward: -42\n","Episode 1700 Total Reward: -42\n","Episode 1800 Total Reward: -112\n","Episode 1900 Total Reward: -81\n","Episode 2000 Total Reward: -53\n","Episode 2100 Total Reward: -41\n","Episode 2200 Total Reward: -56\n","Episode 2300 Total Reward: -46\n","Episode 2400 Total Reward: -49\n","Episode 2500 Total Reward: -47\n","Episode 2600 Total Reward: -56\n","Episode 2700 Total Reward: -106\n","Episode 2800 Total Reward: -161\n","Episode 2900 Total Reward: -53\n","Episode 3000 Total Reward: -46\n","Episode 3100 Total Reward: 9\n","Episode 3200 Total Reward: -59\n","Episode 3300 Total Reward: -60\n","Episode 3400 Total Reward: -15\n","Episode 3500 Total Reward: -58\n","Episode 3600 Total Reward: -59\n","Episode 3700 Total Reward: -2\n","Episode 3800 Total Reward: -104\n","Episode 3900 Total Reward: -61\n","Episode 4000 Total Reward: -53\n","Episode 4100 Total Reward: -54\n","Episode 4200 Total Reward: -58\n","Episode 4300 Total Reward: -51\n","Episode 4400 Total Reward: -12\n","Episode 4500 Total Reward: -52\n","Episode 4600 Total Reward: -118\n","Episode 4700 Total Reward: -51\n","Episode 4800 Total Reward: -48\n","Episode 4900 Total Reward: -57\n","Episode 5000 Total Reward: -60\n","Episode 5100 Total Reward: -60\n","Episode 5200 Total Reward: -56\n","Episode 5300 Total Reward: -44\n","Episode 5400 Total Reward: -53\n","Episode 5500 Total Reward: -53\n","Episode 5600 Total Reward: -1\n","Episode 5700 Total Reward: -49\n","Episode 5800 Total Reward: -55\n","Episode 5900 Total Reward: -10\n","Episode 6000 Total Reward: -56\n","Episode 6100 Total Reward: -102\n","Episode 6200 Total Reward: -44\n","Episode 6300 Total Reward: -3\n","Episode 6400 Total Reward: -42\n","Episode 6500 Total Reward: -43\n","Episode 6600 Total Reward: -4\n","Episode 6700 Total Reward: 1\n","Episode 6800 Total Reward: -54\n","Episode 6900 Total Reward: -49\n","Episode 7000 Total Reward: -56\n","Episode 7100 Total Reward: -119\n","Episode 7200 Total Reward: -51\n","Episode 7300 Total Reward: -107\n","Episode 7400 Total Reward: -52\n","Episode 7500 Total Reward: -68\n","Episode 7600 Total Reward: -51\n","Episode 7700 Total Reward: 0\n","Episode 7800 Total Reward: 1\n","Episode 7900 Total Reward: -59\n","Episode 8000 Total Reward: -53\n","Episode 8100 Total Reward: -46\n","Episode 8200 Total Reward: -120\n","Episode 8300 Total Reward: -3\n","Episode 8400 Total Reward: -50\n","Episode 8500 Total Reward: 2\n","Episode 8600 Total Reward: -53\n","Episode 8700 Total Reward: -43\n","Episode 8800 Total Reward: -110\n","Episode 8900 Total Reward: 1\n","Episode 9000 Total Reward: -59\n","Episode 9100 Total Reward: -44\n","Episode 9200 Total Reward: -105\n","Episode 9300 Total Reward: -119\n","Episode 9400 Total Reward: -116\n","Episode 9500 Total Reward: -57\n","Episode 9600 Total Reward: 0\n","Episode 9700 Total Reward: -51\n","Episode 9800 Total Reward: -60\n","Episode 9900 Total Reward: -53\n","Episode 10000 Total Reward: -117\n","11551  /  15120\n"]}]},{"cell_type":"markdown","metadata":{"id":"fibiFq35Hvkm"},"source":["### Now that we have learned the optimal Q Values we have developed an optimal policy and have no need to train the agent anymore"]},{"cell_type":"code","metadata":{"id":"xPB9WjtBHvkm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634538481798,"user_tz":-480,"elapsed":285,"user":{"displayName":"江詠筑","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14201719718375272034"}},"outputId":"8e111147-9036-4df4-d340-fded70d64a6e"},"source":["state = env.reset()\n","\n","done = None\n","\n","while done != True:\n","    # We simply take the action with the highest Q Value\n","    action = np.argmax(Q[state])\n","    state, reward, done, info = env.step(action)\n","    env.render()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----------------+\n","|O|O| : : : : :G: |\n","|O|O| |O| |O| |O| |\n","|O| : |O| |O| |O| |\n","| : |O|O| : : : : |\n","| |O|O|O|O|O| |O| |\n","| : :\u001b[34;1mR\u001b[0m: : : : |O| |\n","| |O|O|O| |O|O|O| |\n","| |O|\u001b[43m \u001b[0m: : |O| : : |\n","| |O| |O|O|O|\u001b[35mB\u001b[0m|O| |\n","| : : : : : : |O| |\n","| |O| |O| |O| |O| |\n","| : : : : : : |O| |\n","| |O| |O| |O|O|O| |\n","| : : :Y: : : : : |\n","+-----------------+\n","  (North)\n","+-----------------+\n","|O|O| : : : : :G: |\n","|O|O| |O| |O| |O| |\n","|O| : |O| |O| |O| |\n","| : |O|O| : : : : |\n","| |O|O|O|O|O| |O| |\n","| : :\u001b[34;1mR\u001b[0m: : : : |O| |\n","| |O|O|O| |O|O|O| |\n","| |O| :\u001b[43m \u001b[0m: |O| : : |\n","| |O| |O|O|O|\u001b[35mB\u001b[0m|O| |\n","| : : : : : : |O| |\n","| |O| |O| |O| |O| |\n","| : : : : : : |O| |\n","| |O| |O| |O|O|O| |\n","| : : :Y: : : : : |\n","+-----------------+\n","  (East)\n","+-----------------+\n","|O|O| : : : : :G: |\n","|O|O| |O| |O| |O| |\n","|O| : |O| |O| |O| |\n","| : |O|O| : : : : |\n","| |O|O|O|O|O| |O| |\n","| : :\u001b[34;1mR\u001b[0m: : : : |O| |\n","| |O|O|O| |O|O|O| |\n","| |O| : :\u001b[43m \u001b[0m|O| : : |\n","| |O| |O|O|O|\u001b[35mB\u001b[0m|O| |\n","| : : : : : : |O| |\n","| |O| |O| |O| |O| |\n","| : : : : : : |O| |\n","| |O| |O| |O|O|O| |\n","| : : :Y: : : : : |\n","+-----------------+\n","  (East)\n","+-----------------+\n","|O|O| : : : : :G: |\n","|O|O| |O| |O| |O| |\n","|O| : |O| |O| |O| |\n","| : |O|O| : : : : |\n","| |O|O|O|O|O| |O| |\n","| : :\u001b[34;1mR\u001b[0m: : : : |O| |\n","| |O|O|O|\u001b[43m \u001b[0m|O|O|O| |\n","| |O| : : |O| : : |\n","| |O| |O|O|O|\u001b[35mB\u001b[0m|O| |\n","| : : : : : : |O| |\n","| |O| |O| |O| |O| |\n","| : : : : : : |O| |\n","| |O| |O| |O|O|O| |\n","| : : :Y: : : : : |\n","+-----------------+\n","  (North)\n","+-----------------+\n","|O|O| : : : : :G: |\n","|O|O| |O| |O| |O| |\n","|O| : |O| |O| |O| |\n","| : |O|O| : : : : |\n","| |O|O|O|O|O| |O| |\n","| : :\u001b[34;1mR\u001b[0m: :\u001b[43m \u001b[0m: : |O| |\n","| |O|O|O| |O|O|O| |\n","| |O| : : |O| : : |\n","| |O| |O|O|O|\u001b[35mB\u001b[0m|O| |\n","| : : : : : : |O| |\n","| |O| |O| |O| |O| |\n","| : : : : : : |O| |\n","| |O| |O| |O|O|O| |\n","| : : :Y: : : : : |\n","+-----------------+\n","  (North)\n","+-----------------+\n","|O|O| : : : : :G: |\n","|O|O| |O| |O| |O| |\n","|O| : |O| |O| |O| |\n","| : |O|O| : : : : |\n","| |O|O|O|O|O| |O| |\n","| : :\u001b[34;1mR\u001b[0m:\u001b[43m \u001b[0m: : : |O| |\n","| |O|O|O| |O|O|O| |\n","| |O| : : |O| : : |\n","| |O| |O|O|O|\u001b[35mB\u001b[0m|O| |\n","| : : : : : : |O| |\n","| |O| |O| |O| |O| |\n","| : : : : : : |O| |\n","| |O| |O| |O|O|O| |\n","| : : :Y: : : : : |\n","+-----------------+\n","  (West)\n","+-----------------+\n","|O|O| : : : : :G: |\n","|O|O| |O| |O| |O| |\n","|O| : |O| |O| |O| |\n","| : |O|O| : : : : |\n","| |O|O|O|O|O| |O| |\n","| : :\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m: : : : |O| |\n","| |O|O|O| |O|O|O| |\n","| |O| : : |O| : : |\n","| |O| |O|O|O|\u001b[35mB\u001b[0m|O| |\n","| : : : : : : |O| |\n","| |O| |O| |O| |O| |\n","| : : : : : : |O| |\n","| |O| |O| |O|O|O| |\n","| : : :Y: : : : : |\n","+-----------------+\n","  (West)\n","+-----------------+\n","|O|O| : : : : :G: |\n","|O|O| |O| |O| |O| |\n","|O| : |O| |O| |O| |\n","| : |O|O| : : : : |\n","| |O|O|O|O|O| |O| |\n","| : :\u001b[42mR\u001b[0m: : : : |O| |\n","| |O|O|O| |O|O|O| |\n","| |O| : : |O| : : |\n","| |O| |O|O|O|\u001b[35mB\u001b[0m|O| |\n","| : : : : : : |O| |\n","| |O| |O| |O| |O| |\n","| : : : : : : |O| |\n","| |O| |O| |O|O|O| |\n","| : : :Y: : : : : |\n","+-----------------+\n","  (Pickup)\n","+-----------------+\n","|O|O| : : : : :G: |\n","|O|O| |O| |O| |O| |\n","|O| : |O| |O| |O| |\n","| : |O|O| : : : : |\n","| |O|O|O|O|O| |O| |\n","| : :R:\u001b[42m_\u001b[0m: : : |O| |\n","| |O|O|O| |O|O|O| |\n","| |O| : : |O| : : |\n","| |O| |O|O|O|\u001b[35mB\u001b[0m|O| |\n","| : : : : : : |O| |\n","| |O| |O| |O| |O| |\n","| : : : : : : |O| |\n","| |O| |O| |O|O|O| |\n","| : : :Y: : : : : |\n","+-----------------+\n","  (East)\n","+-----------------+\n","|O|O| : : : : :G: |\n","|O|O| |O| |O| |O| |\n","|O| : |O| |O| |O| |\n","| : |O|O| : : : : |\n","| |O|O|O|O|O| |O| |\n","| : :R: :\u001b[42m_\u001b[0m: : |O| |\n","| |O|O|O| |O|O|O| |\n","| |O| : : |O| : : |\n","| |O| |O|O|O|\u001b[35mB\u001b[0m|O| |\n","| : : : : : : |O| |\n","| |O| |O| |O| |O| |\n","| : : : : : : |O| |\n","| |O| |O| |O|O|O| |\n","| : : :Y: : : : : |\n","+-----------------+\n","  (East)\n","+-----------------+\n","|O|O| : : : : :G: |\n","|O|O| |O| |O| |O| |\n","|O| : |O| |O| |O| |\n","| : |O|O| : : : : |\n","| |O|O|O|O|O| |O| |\n","| : :R: : : : |O| |\n","| |O|O|O|\u001b[42m_\u001b[0m|O|O|O| |\n","| |O| : : |O| : : |\n","| |O| |O|O|O|\u001b[35mB\u001b[0m|O| |\n","| : : : : : : |O| |\n","| |O| |O| |O| |O| |\n","| : : : : : : |O| |\n","| |O| |O| |O|O|O| |\n","| : : :Y: : : : : |\n","+-----------------+\n","  (South)\n","+-----------------+\n","|O|O| : : : : :G: |\n","|O|O| |O| |O| |O| |\n","|O| : |O| |O| |O| |\n","| : |O|O| : : : : |\n","| |O|O|O|O|O| |O| |\n","| : :R: : : : |O| |\n","| |O|O|O| |O|O|O| |\n","| |O| : :\u001b[42m_\u001b[0m|O| : : |\n","| |O| |O|O|O|\u001b[35mB\u001b[0m|O| |\n","| : : : : : : |O| |\n","| |O| |O| |O| |O| |\n","| : : : : : : |O| |\n","| |O| |O| |O|O|O| |\n","| : : :Y: : : : : |\n","+-----------------+\n","  (South)\n","+-----------------+\n","|O|O| : : : : :G: |\n","|O|O| |O| |O| |O| |\n","|O| : |O| |O| |O| |\n","| : |O|O| : : : : |\n","| |O|O|O|O|O| |O| |\n","| : :R: : : : |O| |\n","| |O|O|O| |O|O|O| |\n","| |O| :\u001b[42m_\u001b[0m: |O| : : |\n","| |O| |O|O|O|\u001b[35mB\u001b[0m|O| |\n","| : : : : : : |O| |\n","| |O| |O| |O| |O| |\n","| : : : : : : |O| |\n","| |O| |O| |O|O|O| |\n","| : : :Y: : : : : |\n","+-----------------+\n","  (West)\n","+-----------------+\n","|O|O| : : : : :G: |\n","|O|O| |O| |O| |O| |\n","|O| : |O| |O| |O| |\n","| : |O|O| : : : : |\n","| |O|O|O|O|O| |O| |\n","| : :R: : : : |O| |\n","| |O|O|O| |O|O|O| |\n","| |O|\u001b[42m_\u001b[0m: : |O| : : |\n","| |O| |O|O|O|\u001b[35mB\u001b[0m|O| |\n","| : : : : : : |O| |\n","| |O| |O| |O| |O| |\n","| : : : : : : |O| |\n","| |O| |O| |O|O|O| |\n","| : : :Y: : : : : |\n","+-----------------+\n","  (West)\n","+-----------------+\n","|O|O| : : : : :G: |\n","|O|O| |O| |O| |O| |\n","|O| : |O| |O| |O| |\n","| : |O|O| : : : : |\n","| |O|O|O|O|O| |O| |\n","| : :R: : : : |O| |\n","| |O|O|O| |O|O|O| |\n","| |O| : : |O| : : |\n","| |O|\u001b[42m_\u001b[0m|O|O|O|\u001b[35mB\u001b[0m|O| |\n","| : : : : : : |O| |\n","| |O| |O| |O| |O| |\n","| : : : : : : |O| |\n","| |O| |O| |O|O|O| |\n","| : : :Y: : : : : |\n","+-----------------+\n","  (South)\n","+-----------------+\n","|O|O| : : : : :G: |\n","|O|O| |O| |O| |O| |\n","|O| : |O| |O| |O| |\n","| : |O|O| : : : : |\n","| |O|O|O|O|O| |O| |\n","| : :R: : : : |O| |\n","| |O|O|O| |O|O|O| |\n","| |O| : : |O| : : |\n","| |O| |O|O|O|\u001b[35mB\u001b[0m|O| |\n","| : :\u001b[42m_\u001b[0m: : : : |O| |\n","| |O| |O| |O| |O| |\n","| : : : : : : |O| |\n","| |O| |O| |O|O|O| |\n","| : : :Y: : : : : |\n","+-----------------+\n","  (South)\n","+-----------------+\n","|O|O| : : : : :G: |\n","|O|O| |O| |O| |O| |\n","|O| : |O| |O| |O| |\n","| : |O|O| : : : : |\n","| |O|O|O|O|O| |O| |\n","| : :R: : : : |O| |\n","| |O|O|O| |O|O|O| |\n","| |O| : : |O| : : |\n","| |O| |O|O|O|\u001b[35mB\u001b[0m|O| |\n","| : : :\u001b[42m_\u001b[0m: : : |O| |\n","| |O| |O| |O| |O| |\n","| : : : : : : |O| |\n","| |O| |O| |O|O|O| |\n","| : : :Y: : : : : |\n","+-----------------+\n","  (East)\n","+-----------------+\n","|O|O| : : : : :G: |\n","|O|O| |O| |O| |O| |\n","|O| : |O| |O| |O| |\n","| : |O|O| : : : : |\n","| |O|O|O|O|O| |O| |\n","| : :R: : : : |O| |\n","| |O|O|O| |O|O|O| |\n","| |O| : : |O| : : |\n","| |O| |O|O|O|\u001b[35mB\u001b[0m|O| |\n","| : : : :\u001b[42m_\u001b[0m: : |O| |\n","| |O| |O| |O| |O| |\n","| : : : : : : |O| |\n","| |O| |O| |O|O|O| |\n","| : : :Y: : : : : |\n","+-----------------+\n","  (East)\n","+-----------------+\n","|O|O| : : : : :G: |\n","|O|O| |O| |O| |O| |\n","|O| : |O| |O| |O| |\n","| : |O|O| : : : : |\n","| |O|O|O|O|O| |O| |\n","| : :R: : : : |O| |\n","| |O|O|O| |O|O|O| |\n","| |O| : : |O| : : |\n","| |O| |O|O|O|\u001b[35mB\u001b[0m|O| |\n","| : : : : :\u001b[42m_\u001b[0m: |O| |\n","| |O| |O| |O| |O| |\n","| : : : : : : |O| |\n","| |O| |O| |O|O|O| |\n","| : : :Y: : : : : |\n","+-----------------+\n","  (East)\n","+-----------------+\n","|O|O| : : : : :G: |\n","|O|O| |O| |O| |O| |\n","|O| : |O| |O| |O| |\n","| : |O|O| : : : : |\n","| |O|O|O|O|O| |O| |\n","| : :R: : : : |O| |\n","| |O|O|O| |O|O|O| |\n","| |O| : : |O| : : |\n","| |O| |O|O|O|\u001b[35mB\u001b[0m|O| |\n","| : : : : : :\u001b[42m_\u001b[0m|O| |\n","| |O| |O| |O| |O| |\n","| : : : : : : |O| |\n","| |O| |O| |O|O|O| |\n","| : : :Y: : : : : |\n","+-----------------+\n","  (East)\n","+-----------------+\n","|O|O| : : : : :G: |\n","|O|O| |O| |O| |O| |\n","|O| : |O| |O| |O| |\n","| : |O|O| : : : : |\n","| |O|O|O|O|O| |O| |\n","| : :R: : : : |O| |\n","| |O|O|O| |O|O|O| |\n","| |O| : : |O| : : |\n","| |O| |O|O|O|\u001b[35m\u001b[42mB\u001b[0m\u001b[0m|O| |\n","| : : : : : : |O| |\n","| |O| |O| |O| |O| |\n","| : : : : : : |O| |\n","| |O| |O| |O|O|O| |\n","| : : :Y: : : : : |\n","+-----------------+\n","  (North)\n","+-----------------+\n","|O|O| : : : : :G: |\n","|O|O| |O| |O| |O| |\n","|O| : |O| |O| |O| |\n","| : |O|O| : : : : |\n","| |O|O|O|O|O| |O| |\n","| : :R: : : : |O| |\n","| |O|O|O| |O|O|O| |\n","| |O| : : |O| : : |\n","| |O| |O|O|O|\u001b[35m\u001b[42mB\u001b[0m\u001b[0m|O| |\n","| : : : : : : |O| |\n","| |O| |O| |O| |O| |\n","| : : : : : : |O| |\n","| |O| |O| |O|O|O| |\n","| : : :Y: : : : : |\n","+-----------------+\n","  (Dropoff)\n"]}]},{"cell_type":"markdown","metadata":{"id":"cuiYyMDyzIk8"},"source":["### Part B: 2 points\n","\n","Hints:\n","* implement a function for the learned the optimal Q Values for a given state (124)\n","* You could use ACTIONS = [\"South\", \"North\", \"East\", \"West\", \"Pickup\", \"Dropoff\"] for out_1"]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"09a6a0c553269caacbfa15e7675aa7a5","grade":false,"grade_id":"cell-4d504d9a0050eab6","locked":false,"schema_version":3,"solution":true,"task":false},"id":"qPpQpl3fzIk8"},"source":["def test_q_learning(env , Q , state):\n","    '''\n","    out_1: the sequence of actions\n","    out_2: total reward\n","    out_3: total steps\n","    '''\n","\n","    ACTIONS = [\"South\" , \"North\" , \"East\" , \"West\" , \"Pickup\" , \"Dropoff\"]\n","    action_seq = []\n","    tot_reward = 0\n","    tot_steps = 0\n","    done = None\n","    while done != True:\n","      action = np.argmax(Q[state])\n","      state , reward , done , info = env.step(action)\n","      action_seq.append(ACTIONS[action])\n","      tot_steps += 1\n","      tot_reward += reward\n","    return action_seq , tot_reward , tot_steps"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"id":"94hfU8-dHvko","nbgrader":{"cell_type":"code","checksum":"6c29558ef8b91c4b559e5855aab82461","grade":true,"grade_id":"cell-35aa4afbf5de35e6","locked":true,"points":2,"schema_version":3,"solution":false,"task":false},"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634538492556,"user_tz":-480,"elapsed":285,"user":{"displayName":"江詠筑","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14201719718375272034"}},"outputId":"a3db046f-99a4-4c36-ea28-eda7c2840026"},"source":["\n","env.s = 124\n","env.render()\n","out_1 , out_2 , out_3 = test_q_learning(env , Q , env.s)\n","assert out_1 == ['East', 'Pickup', 'West', 'South', 'South', 'South', 'South', 'South', 'West', 'West', 'West', 'West', 'Dropoff']\n","assert out_2 == 8\n","assert out_3 == 13\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----------------+\n","|O|O| : : : :\u001b[43m \u001b[0m:\u001b[34;1mG\u001b[0m: |\n","|O|O| |O| |O| |O| |\n","|O| : |O| |O| |O| |\n","| : |O|O| : : : : |\n","| |O|O|O|O|O| |O| |\n","| : :\u001b[35mR\u001b[0m: : : : |O| |\n","| |O|O|O| |O|O|O| |\n","| |O| : : |O| : : |\n","| |O| |O|O|O|B|O| |\n","| : : : : : : |O| |\n","| |O| |O| |O| |O| |\n","| : : : : : : |O| |\n","| |O| |O| |O|O|O| |\n","| : : :Y: : : : : |\n","+-----------------+\n","  (Dropoff)\n"]}]},{"cell_type":"code","metadata":{"id":"KP24qvDpzIk8"},"source":[""],"execution_count":null,"outputs":[]}]}